## ğŸ“ˆ Kafka Stock Market Data Streamer

A real-time data streaming project simulating stock market data using Kafka and storing batched JSON records in Amazon S3. Built for practicing ingestion pipelines, message brokers, and data lake integration.

---

### ğŸš€ Tech Stack
- **Python**
- **Kafka** (`kafka-python`)
- **Pandas**
- **s3fs** (for S3 access)
- **AWS S3 / MinIO**

---

### ğŸ§  Project Overview

This project simulates a live stock market feed from a CSV file, streams each row as a Kafka message, and uses a Kafka consumer to batch and store data in an S3 bucket.

#### âœ”ï¸ Producer:
- Reads a local CSV (`indexProcessed.csv`)
- Converts each row to JSON and sends to Kafka topic `demo_test`

#### âœ”ï¸ Consumer:
- Listens to `demo_test`
- Buffers messages into batches (default: 10)
- Writes batched data to S3 in `.json` format

---

### ğŸ“ Folder Structure

```
kafka_stock_market/
â”œâ”€â”€ producer.py        # Kafka producer sending CSV rows
â”œâ”€â”€ consumer.py        # Kafka consumer storing batched data to S3
â”œâ”€â”€ data/
â”‚   â””â”€â”€ indexProcessed.csv  # Simulated stock market data
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ README.md
```

---

### âš™ï¸ How to Run

1. **Start Kafka & Zookeeper locally or via Docker**

```bash
docker-compose up -d
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Run the Producer**
```bash
python producer.py
```

4. **Run the Consumer**
```bash
python consumer.py
```

---

### ğŸ”ª Features to Extend
- Switch from JSON to Parquet using `pyarrow`
- Integrate with Spark Structured Streaming for large-scale ingestion
- Implement schema validation using `pydantic`
- Add email or Slack alerts for consumer errors

---

### ğŸ“š Learning Outcomes

- Kafka Producer/Consumer design
- Data pipeline buffering strategy
- S3 write optimization
- Streaming data architecture concepts
